{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f23156e-65fa-4148-a04b-da04801972f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a href=\"https://mingxia.web.unc.edu/\" target=\"_parent\"><img src=\"https://mingxia.web.unc.edu/wp-content/uploads/sites/12411/2020/12/logo_MagicLab-horizontal-4.png\" alt=\"MAGIC Lab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc9186-ed0b-48d4-a73d-9ed35a1c6bd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Downstream classification model finetuning based on pretrained pretext model of BAR**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e927f-031a-4388-a6f0-7701f4d8abf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Loading required libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1cf6f1-550f-4a14-8aec-c3696c2fd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "import sys, argparse\n",
    "import enum\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import multiprocessing\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pylab as pl\n",
    "import logging\n",
    "import shutil\n",
    "import tempfile\n",
    "import gzip\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from IPython import display\n",
    "from tqdm import trange, tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "import pprint\n",
    "import torchio as tio\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "from torch.nn import L1Loss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import svm\n",
    "\n",
    "from neuroCombat import neuroCombat\n",
    "\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, ImageDataset\n",
    "from monai.networks.nets import VarAutoEncoder,ViTAutoEnc, AutoEncoder, Classifier\n",
    "from monai.networks.layers.convutils import calculate_out_shape, same_padding\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.metrics import compute_hausdorff_distance, HausdorffDistanceMetric\n",
    "from monai.losses import ContrastiveLoss, DiceLoss, DiceCELoss\n",
    "from monai.transforms import (\n",
    "    ConvertToMultiChannelBasedOnBratsClasses,\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "\n",
    "#    AddChannel,\n",
    "#    Compose,\n",
    "#    RandRotate90,\n",
    "#    Resize,\n",
    "#    ScaleIntensity,\n",
    "#    EnsureType\n",
    "    AddChannelD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    ScaleIntensityD,\n",
    "    EnsureTypeD,\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61b50d-c742-4404-9c6c-1d4a29021f5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Global Setting**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd31e76-8597-436b-b2ad-8bb05b520111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "set_determinism(seed=0)\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "pretrained_path = './pretrained/'\n",
    "trained_path = './models/'\n",
    "logdir_path = os.path.normpath('./log/')\n",
    "if os.path.exists(logdir_path)==False:\n",
    "    os.mkdir(logdir_path)\n",
    "if os.path.exists(pretrained_path)==False:\n",
    "    os.mkdir(pretrained_path)\n",
    "if os.path.exists(trained_path)==False:\n",
    "    os.mkdir(trained_path)\n",
    "\n",
    "modelname = 'AEclf256';#resnet18,resnet34,resnet50,efficientnet-b0,DenseNet121\n",
    "pretrained = False\n",
    "downsampled = False\n",
    "samplespace = 1\n",
    "if downsampled:\n",
    "    samplespace = 2 #2,4,8\n",
    "max_epochs = 2\n",
    "val_interval = 5\n",
    "kfold = 5\n",
    "categories = 2\n",
    "Combat = False\n",
    "if pretrained:\n",
    "    savedir = trained_path+'Pretrained_'\n",
    "else:\n",
    "    savedir = trained_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c20ec-7d44-44b6-a390-5e3b7181ce61",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Define Training Transforms**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9709957e-18ae-452d-af47-e51aedda1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, segs, labels, augment=False):\n",
    "        subjects = []            \n",
    "        for (image, seg, label) in zip(images, segs, labels):\n",
    "            #print(image_path,label)\n",
    "            subject = tio.Subject(\n",
    "                mri=tio.ScalarImage(image),\n",
    "                #rec=tio.ScalarImage(recon),\n",
    "                #seg=tio.LabelMap(seg),\n",
    "                #roi=tio.LabelMap(roi),\n",
    "                labels=int(label),\n",
    "            )\n",
    "            subjects.append(subject)\n",
    "        self.transform()\n",
    "        if augment:\n",
    "            self.dataset = tio.SubjectsDataset(subjects, transform=self.aug_transform)\n",
    "        else:\n",
    "            self.dataset = tio.SubjectsDataset(subjects, transform=self.preproc_transform)\n",
    "            \n",
    "    def transform(self):\n",
    "        #mni = tio.datasets.Colin27()\n",
    "        get_foreground = tio.ZNormalization.mean\n",
    "        ADNI_landmarks = np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.64676642, 26.44374401, 47.65044424, 78.28128123, 100.])#ADNI_iBEAT\n",
    "        landmarks_dict = {'mri': ADNI_landmarks}\n",
    "        preprocess = tio.Compose([\n",
    "            tio.ToCanonical(),\n",
    "            #tio.CropOrPad((176, 208, 176)),                                              # tight crop around brain\n",
    "            #tio.RescaleIntensity(percentiles=(0.,99.5), out_min_max=(0, 1.0)),\n",
    "        ])\n",
    "        augment = tio.Compose([\n",
    "            tio.RandomAffine(scales=0.1,degrees=20,translation=5,isotropic=True,center='image'),       # random affine\n",
    "        ])\n",
    "\n",
    "        self.aug_transform = tio.Compose([preprocess, augment])\n",
    "        self.preproc_transform = preprocess\n",
    "\n",
    "\n",
    "def get_loader(imagepaths,segpaths, labels, batch_size=1, augment=False):\n",
    "    dataset = MDD_Dataset(images=imagepaths,segs=segpaths, labels=labels, augment=augment)\n",
    "    #batch_size = 6\n",
    "    if augment:\n",
    "        #batch_size = 6\n",
    "        loader = DataLoader(dataset.dataset,batch_size=batch_size,num_workers=batch_size,shuffle=True,pin_memory=pin_memory,drop_last=False)\n",
    "    else:\n",
    "        loader = DataLoader(dataset.dataset,batch_size=batch_size,num_workers=batch_size,shuffle=False,pin_memory=pin_memory)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681998d-9c54-4201-bdb3-39a2e44ffc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Model Defination**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e537e2-c27b-4604-857f-ccb75f21c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderClassifier(AutoEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_shape: Sequence[int],\n",
    "        out_channels: int,\n",
    "        num_classes: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        out_channels2: int = 4,\n",
    "        kernel_size: Union[Sequence[int], int] = 3,\n",
    "        up_kernel_size: Union[Sequence[int], int] = 3,\n",
    "        num_res_units: int = 0,\n",
    "        inter_channels: Optional[list] = None,\n",
    "        inter_dilations: Optional[list] = None,\n",
    "        num_inter_units: int = 2,\n",
    "        act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
    "        norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
    "        bias: bool = True,\n",
    "        use_sigmoid: bool = False,\n",
    "        use_softmax: bool = True,\n",
    "        latent_size: int = 64,\n",
    "    ) -> None:\n",
    "\n",
    "        self.in_channels, *self.in_shape = in_shape\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.final_size = np.asarray(self.in_shape, dtype=int)\n",
    "\n",
    "        super().__init__(\n",
    "            spatial_dims,\n",
    "            self.in_channels,\n",
    "            out_channels,\n",
    "            channels,\n",
    "            strides,\n",
    "            kernel_size,\n",
    "            up_kernel_size,\n",
    "            num_res_units,\n",
    "            inter_channels,\n",
    "            inter_dilations,\n",
    "            num_inter_units,\n",
    "            act,\n",
    "            norm,\n",
    "            dropout,\n",
    "            bias,\n",
    "        )\n",
    "\n",
    "        padding = same_padding(self.kernel_size)\n",
    "\n",
    "        for s in strides:\n",
    "            self.final_size = calculate_out_shape(self.final_size, self.kernel_size, s, padding)  # type: ignore\n",
    "\n",
    "        linear_size = int(np.product(self.final_size)) * self.encoded_channels\n",
    "        \n",
    "        self.clf = Classifier(in_shape = (self.channels[-1], *self.final_size), \n",
    "                              classes = num_classes, \n",
    "                              channels = (256,),\n",
    "                              strides = (2,), \n",
    "                              #num_res_units = 0,\n",
    "                              norm='INSTANCE', \n",
    "                              dropout=None, \n",
    "                              last_act=None)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encode(x)\n",
    "        x = self.intermediate(x)\n",
    "        y = self.clf(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec97f05-7bf3-446b-b620-ce61f820e1e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training Process Defination**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccc2591-e139-4b44-aab4-6456c0104dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Loss  = torch.nn.L1Loss(reduction='sum')\n",
    "MSELoss = torch.nn.MSELoss(reduction='sum')\n",
    "BCELoss = torch.nn.BCELoss(reduction='sum')\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dice_loss = DiceLoss(include_background=True ,to_onehot_y=True, softmax=True)\n",
    "\n",
    "def train(model, max_epochs, learning_rate, savename):\n",
    "    # Create optimiser\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=1e-5)\n",
    "    model.to(device)\n",
    "    avg_train_losses = []\n",
    "    avg_train_dice_losses =[]\n",
    "    avg_train_mse_losses = []\n",
    "    avg_train_kld_losses = []\n",
    "    test_losses = []\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    threshhold = 0.6\n",
    "\n",
    "    \n",
    "    t = trange(max_epochs, leave=True, desc=\"step: 0,  epoch: 0,   average train loss: ?, test loss: ?\")\n",
    "    \n",
    "    for epoch in t:\n",
    "        model.train()\n",
    "        mse_losses = []\n",
    "        dice_losses = []\n",
    "        kld_losses = []\n",
    "        epoch_losses = []\n",
    "        epoch_loss = 0\n",
    "        mse_loss = 0\n",
    "        kld_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step +=1\n",
    "            inputs = batch_data['mri'][tio.DATA].to(device).float()\n",
    "            labels = batch_data['labels'].to(device)\n",
    "            #print(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            t.set_description(f\"train step: {step}\")\n",
    "        scheduler.step()\n",
    "        avg_train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        if True and (epoch+1)%val_interval == 0:\n",
    "            # Test\n",
    "            test_pred = []\n",
    "            test_prob = []\n",
    "            test_label = []\n",
    "            test_predict= []\n",
    "            model.eval()\n",
    "            test_loss = []\n",
    "            step = 0\n",
    "            for test_data in test_loader:\n",
    "                step +=1\n",
    "                test_inputs = test_data['mri'][tio.DATA].to(device)\n",
    "                test_labels = test_data['labels'].to(device)\n",
    "                with torch.no_grad():\n",
    "                    test_outputs = model(test_inputs.float())\n",
    "                test_loss.append(loss_function(test_outputs, test_labels).item())\n",
    "                if (epoch+1) >= 30:\n",
    "                    outprob = F.softmax(test_outputs, dim=1)\n",
    "                    test_prob.append(outprob)\n",
    "                    test_pred.append(outprob.max(dim=1)[1])\n",
    "                    test_predict.append(torch.where(outprob>0.5,torch.ones_like(outprob),torch.zeros_like(outprob)))\n",
    "                    test_label.append(test_labels)\n",
    "                t.set_description(f\"test  step: {step}\")\n",
    "            test_losses.append(np.mean(test_loss))\n",
    "\n",
    "            if (epoch+1) > 30:\n",
    "                y_prob = torch.cat(test_prob, dim=0).cpu().detach().numpy()\n",
    "                #y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "                y_true = torch.cat(test_label, dim=0).cpu().detach().numpy()\n",
    "                y_pred =  np.array(torch.cat(test_predict, dim=0).cpu().detach().numpy()[...,1],dtype=int)\n",
    "                epoch_report = classification_report(y_true, y_pred, output_dict = True,target_names=['CN', 'AD'],zero_division=0)\n",
    "\n",
    "                metric = epoch_report['accuracy']\n",
    "                if ((metric > best_metric+0.005) and (metric > threshhold)) or ((epoch+1)%10 == 0):\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    best_model_name = savename+'_epoch%02i_acc%.2f.pth' % (best_metric_epoch,best_metric)\n",
    "                    torch.save(model.state_dict(), best_model_name)\n",
    "                    print(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "                if (metric > 0.9):\n",
    "                    break\n",
    "        if len(test_losses)>0:\n",
    "            t.set_postfix(avg_train_losses=avg_train_losses[-1], avg_test_losses=test_losses[-1])\n",
    "    return model, best_model_name, avg_train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874a9a9-238a-4f4b-995d-12ac414000d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Testing Process Defination**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03a04de-1ce3-4c5d-9231-5da42edf10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    min_v_loss = np.inf\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "\n",
    "\n",
    "    print(\"-\" * 10)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    prob = []\n",
    "    label = []\n",
    "    predict= []\n",
    "\n",
    "    num_correct = 0.0\n",
    "    metric_count = 0\n",
    "    for test_data in tqdm(test_loader):\n",
    "        test_inputs = test_data['mri'][tio.DATA].to(device)\n",
    "        test_labels = test_data['labels']\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(test_inputs.float())\n",
    "        outprob = F.softmax(test_outputs, dim=1)\n",
    "        prob.append(outprob)\n",
    "        pred.append(outprob.max(dim=1)[1])\n",
    "        predict.append(torch.where(outprob>0.5,torch.ones_like(outprob),torch.zeros_like(outprob)))\n",
    "        label.append(test_labels)\n",
    "\n",
    "    y_prob = torch.cat(prob, dim=0).cpu().detach().numpy()\n",
    "    #y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    y_pred =  np.array(torch.cat(predict, dim=0).cpu().detach().numpy()[...,1],dtype=int)\n",
    "\n",
    "    AUC = roc_auc_score(y_true,y_prob[:,1]) *100\n",
    "    MCC = matthews_corrcoef(y_true, y_pred)*100\n",
    "    confus_mtrx = confusion_matrix(y_true, y_pred).ravel() #sample_weight=sw ravel:flatten\n",
    "    #SPE  = confus_mtrx[0]/(confus_mtrx[0]+confus_mtrx[1])\n",
    "    epoch_report = classification_report(y_true, y_pred, output_dict = True,target_names=['CND', 'CI'],zero_division=0)\n",
    "    return AUC, MCC, epoch_report, confus_mtrx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d645522-b7ff-4adb-9f16-ba977dbab562",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Model Training and Test**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b165121-9504-45eb-8021-161af61c9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained pretext model!\n",
      "loading pretrained auto encoder:./models/RecSeg_AE512_seg_dice_epoch30_loss7.86.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_261114/2001387384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpre_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreAE_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading pretrained auto encoder:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpre_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "im_shape = (1,176,208,176)\n",
    "test_AUC =[]\n",
    "test_ACC =[]\n",
    "test_PRE =[]\n",
    "test_SEN =[]\n",
    "test_SPE =[]\n",
    "test_F1s =[]\n",
    "test_MCC =[]\n",
    "\n",
    "finetuned = True\n",
    "preAEmodelname = 'RecSeg'\n",
    "savefile = savedir+modelname+'_'+preAEmodelname+'_fintune_cn_ci'\n",
    "logfile = savefile.replace(trained_path,logdir_path)\n",
    "\n",
    "out_ch = 1        \n",
    "if preAEmodelname == 'RecSeg':\n",
    "    preAE_model = './models/'+'RecSeg_AE512_seg_dice_epoch30_loss7.86'+'.pth'\n",
    "    out_ch = 4    \n",
    "if preAEmodelname == 'RecInput':\n",
    "    preAE_model = './models/'+'RecInput_AE512_mri_l1loss_epoch30_loss170485.29'+'.pth'\n",
    "\n",
    "for k in range(5):\n",
    "    model = AutoEncoderClassifier(\n",
    "        spatial_dims=3,\n",
    "        in_shape=im_shape,\n",
    "        num_classes=categories,\n",
    "        out_channels=out_ch,\n",
    "        channels=(64,128,256,512),\n",
    "        strides=(2,2,2,2),\n",
    "        inter_channels=(512, 512),\n",
    "        num_inter_units=2,\n",
    "    )    \n",
    "\n",
    "    print('Loading pretrained pretext model!')\n",
    "    if finetuned:\n",
    "        pre_model = preAE_model\n",
    "        print('loading pretrained auto encoder:'+pre_model)\n",
    "        model.load_state_dict(torch.load(pre_model), strict = False)\n",
    "        model_dict = model.state_dict()\n",
    "        for name, p in model.named_parameters():\n",
    "            if name.startswith('encode') or name.startswith('intermediate'): #or name.startswith('decode'):\n",
    "                p.requires_grad = False\n",
    "\n",
    "    train_listfile = './data/LLD/LLD_labels_cn_ci_train_balenced_'+str(k)+'.csv'\n",
    "    test_listfile = './data/LLD/LLD_labels_cn_ci_test_'+str(k)+'.csv'\n",
    "    train_csv_data = pd.read_csv(train_listfile)  # 读取训练数据\n",
    "    test_csv_data = pd.read_csv(test_listfile)  # 读取训练数据\n",
    "\n",
    "    train_mri_path_list = train_csv_data['mripath'].values.tolist()\n",
    "    train_seg_path_list = train_mri_path_list\n",
    "    train_label_list    = train_csv_data['labels'].values.tolist()\n",
    "    test_mri_path_list = test_csv_data['mripath'].values.tolist()\n",
    "    test_seg_path_list = test_mri_path_list\n",
    "    test_label_list    = test_csv_data['labels'].values.tolist()\n",
    "\n",
    "    train_loader = get_loader(train_mri_path_list,train_seg_path_list,train_label_list, batch_size=4, augment=True)\n",
    "    test_loader  = get_loader(test_mri_path_list,test_seg_path_list,test_label_list, batch_size=2)\n",
    "\n",
    "    print('Training start!')\n",
    "\n",
    "    max_epochs = 90\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    savename = savefile+'-fold'+str(k)\n",
    "    model, best_model_name, avg_train_losses, test_losses = train(model, max_epochs, learning_rate, savename)\n",
    "\n",
    "    print(\"Training Finish!\")\n",
    "\n",
    "    print('Testing start!')\n",
    "\n",
    "    AUC, MCC, epoch_report, confus_mtrx = test(model, test_loader)\n",
    "    #print(epoch_report)\n",
    "    ACC = epoch_report['accuracy']*100\n",
    "    SPE = epoch_report['CND']['recall']*100\n",
    "    PRE = epoch_report['CI']['precision']*100\n",
    "    SEN = epoch_report['CI']['recall']*100\n",
    "    F1s = epoch_report['CI']['f1-score']*100\n",
    "    test_AUC.append(AUC)\n",
    "    test_ACC.append(ACC)\n",
    "    test_PRE.append(PRE)\n",
    "    test_SEN.append(SEN)\n",
    "    test_SPE.append(SPE)\n",
    "    test_F1s.append(F1s)\n",
    "    test_MCC.append(MCC)\n",
    "    with open(logfile+'.txt', 'a') as f:\n",
    "        f.writelines('fold'+str(k)+'\\n')\n",
    "        f.writelines(best_model_name+'\\n')\n",
    "        f.writelines(f'AUC:{AUC:.2f}, ACC:{ACC:.2f}, PRE:{PRE:.2f}, SEN:{SEN:.2f}, SPE:{SPE:.2f}, F1S:{F1s:.2f}, MCC:{MCC:.2f}\\n')\n",
    "        \n",
    "    print(f'AUC:{AUC:.2f}, ACC:{ACC:.2f}, PRE:{PRE:.2f}, SEN:{SEN:.2f}, SPE:{SPE:.2f}, F1S:{F1s:.2f}, MCC:{MCC:.2f}')\n",
    "    print(f'{AUC:.2f}\\t{ACC:.2f}\\t{PRE:.2f}\\t{SEN:.2f}\\t{SPE:.2f}\\t{F1s:.2f}\\t{MCC:.2f}')\n",
    "\n",
    "    print('Testing finish!')\n",
    "mean_AUC =round(np.mean(test_AUC),2)\n",
    "mean_ACC =round(np.mean(test_ACC),2)\n",
    "mean_PRE =round(np.mean(test_PRE),2)\n",
    "mean_SEN =round(np.mean(test_SEN),2)\n",
    "mean_SPE =round(np.mean(test_SPE),2)\n",
    "mean_F1s =round(np.mean(test_F1s),2)\n",
    "mean_MCC =round(np.mean(test_MCC),2)\n",
    "\n",
    "std_AUC =round(np.std(test_AUC,ddof=1),2)\n",
    "std_ACC =round(np.std(test_ACC,ddof=1),2)\n",
    "std_PRE =round(np.std(test_PRE,ddof=1),2)\n",
    "std_SEN =round(np.std(test_SEN,ddof=1),2)\n",
    "std_SPE =round(np.std(test_SPE,ddof=1),2)\n",
    "std_F1s =round(np.std(test_F1s,ddof=1),2)\n",
    "std_MCC =round(np.std(test_MCC,ddof=1),2)\n",
    "log1 = f'AUC:{mean_AUC}\\xB1{std_AUC}, ACC:{mean_ACC}\\xB1{std_ACC}, PRE:{mean_PRE}\\xB1{std_PRE}, SEN:{mean_SEN}\\xB1{std_SEN}, SPE:{mean_SPE}\\xB1{std_SPE}, F1S:{mean_F1s}\\xB1{std_F1s}, MCC:{mean_MCC}\\xB1{std_MCC}\\n'\n",
    "log2 = f'{mean_AUC}\\xB1{std_AUC}\\t{mean_ACC}\\xB1{std_ACC}\\t{mean_PRE}\\xB1{std_PRE}\\t{mean_SEN}\\xB1{std_SEN}\\t{mean_SPE}\\xB1{std_SPE}\\t{mean_F1s}\\xB1{std_F1s}\\t{mean_MCC}\\xB1{std_MCC}\\n'\n",
    "              \n",
    "log3 = f'&{mean_AUC}\\xB1{std_AUC} &{mean_ACC}\\xB1{std_ACC} &{mean_SEN}\\xB1{std_SEN} &{mean_SPE}\\xB1{std_SPE} &{mean_F1s}\\xB1{std_F1s}\\n'\n",
    "log4 = f'&{mean_AUC:.1f}({std_AUC:.1f}) &{mean_ACC:.1f}({std_ACC:.1f}) &{mean_SEN:.1f}({std_SEN:.1f}) &{mean_SPE:.1f}({std_SPE:.1f}) &{mean_F1s:.1f}({std_F1s:.1f})\\n'\n",
    "              \n",
    "with open(logfile+'.txt', 'a') as f:\n",
    "    f.writelines(log1)\n",
    "    f.writelines(log2)\n",
    "    f.writelines(log3)\n",
    "    f.writelines(log4)\n",
    "\n",
    "print(log1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f491d-9d96-434f-8de8-3f7d64d53160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
